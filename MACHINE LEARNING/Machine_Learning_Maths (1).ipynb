{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eob29lwluG2x"
   },
   "source": [
    "# Machine Learning from Scratch: Linear & Logistic Regression\n",
    "---\n",
    "This assignment notebook is designed to help you understand the mathematics behind two fundamental ML algorithms: Linear Regression and Logistic Regression.<br>\n",
    "You will:\n",
    "- Derive cost functions (MSE & Log Loss)\n",
    "- Implement Gradient Descent from scratch\n",
    "- Apply the models to simple datasets\n",
    "- Understand how theory translates into practice\n",
    "\n",
    "*(Let's begin üöÄ)*\n",
    "\n",
    "---\n",
    "\n",
    "## Linear Regression (predicting numbers)\n",
    "\n",
    "### The Idea  \n",
    "Now We use linear regression when we want to **fit a straight line** to data. It is a simple model used for regression problems in ML.  \n",
    "Remember from your MTH courses the Equation of a straight line is: $\\hat y = w \\cdot x + b$ <br> where:\n",
    "- \\(w\\) = slope or gradient (how steep the line is)  \n",
    "- \\(b\\) = intercept (where it cuts the y-axis)  \n",
    "- \\(x\\) = features or independent variable\n",
    "- \\(y\\) = labels or dependent variable\n",
    "\n",
    "A linear regression model basically learns what rules map inputs (x) to outputs (y). In this case it learns the appropriate w and b that explains the relationship between x and y.\n",
    "\n",
    "### Training Data for Question\n",
    "\n",
    "| x | y |\n",
    "|---|---|\n",
    "| 1 | 2 |\n",
    "| 2 | 4 |\n",
    "| 3 | 6 |\n",
    "| 4 | 8 |\n",
    "\n",
    "This follows the rule \\(y = 2x\\).  \n",
    "\n",
    "---\n",
    "\n",
    "# **1. Question**\n",
    "We start with the formula:\n",
    "$\\hat y = w \\cdot x + b$ <br>\n",
    "your task is to write the formula in code and test that it produces correct labels <br>\n",
    "Stater code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "TK31M4ja7Ryj"
   },
   "outputs": [],
   "source": [
    "def predict(x, w, b):\n",
    "    pass\n",
    "\n",
    "# Try different values\n",
    "print(predict(2, 1, 0))   # expect 2\n",
    "print(predict(2, 2, 0))   # expect 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AxZJ1MVazMAv"
   },
   "source": [
    "---\n",
    "\n",
    "### Understanding the Loss (or Cost) Function: Mean Squared Error (MSE)\n",
    "\n",
    "Now that we have drawn a line, how do we know if it is a **good line** or a **bad line**?  \n",
    "We need a way to **measure how wrong** our line is compared to the actual data.\n",
    "\n",
    "Think of it this way:\n",
    "- For each point, we can look at the **real value** (`y`) and the **predicted value** (`≈∑`).\n",
    "- If our line is perfect, the real value and the predicted value will be the same.\n",
    "- If our line is not perfect, there will be some **difference (error)**.\n",
    "\n",
    "We square this difference (so negatives don‚Äôt cancel out) and then take the **average of all the squared errors**.  \n",
    "This gives us a single number that tells us how \"bad\" or \"good\" our line is.\n",
    "\n",
    "**Formula (don‚Äôt worry, it‚Äôs just saying what we explained):**\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{m} \\sum (y - \\hat y)^2\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- \\( y \\) = the real value from the data  \n",
    "- \\( ≈∑ \\) = the predicted value from our line  \n",
    "- \\( m \\) = the number of data points  \n",
    "\n",
    "---\n",
    "\n",
    "# **2. Question**\n",
    "\n",
    "Calculating the Mean Squared Error (MSE)\n",
    "\n",
    "Now that you wrote the prediction formula in Question 1, let‚Äôs see **how wrong your line is** compared to the real data.  \n",
    "\n",
    "Remember, the MSE tells us the **average squared difference** between the predicted values (`≈∑`) and the actual values (`y`):\n",
    "\n",
    "- Use the same `predict()` function you wrote in Question 1.\n",
    "- Use different **weights (`w`) and intercept (`b`) you have tried** to get appropriate outputs for the Training Data, also use other (`w`) and (`b`) and observe how the MSE changes.\n",
    "- Then calculate the MSE.\n",
    "\n",
    "**Starter code**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qq6NBVsg7YqG"
   },
   "outputs": [],
   "source": [
    "# Training data\n",
    "x_data = [1, 2, 3, 4]\n",
    "y_data = [2, 4, 6, 8]\n",
    "\n",
    "# Use your calculated w and b for the data given\n",
    "w = None  # replace with your chosen value\n",
    "b = None   # replace with your chosen value\n",
    "\n",
    "# Step 1: Make predictions using your line (no need to edit this)\n",
    "y_pred = [predict(xi, w, b) for xi in x_data]\n",
    "\n",
    "# Step 2: Calculate MSE (no need to edit this)\n",
    "def mse(x_data, y_data, w, b):\n",
    "    errors = [(yi - predict(xi, w, b))**2 for xi, yi in zip(x_data, y_data)]\n",
    "    return sum(errors)/len(x_data)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse(x_data, y_data, w, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q7NTor5h1yYW"
   },
   "source": [
    "### Gradient Descent (Learning the Best Line)\n",
    "\n",
    "So far we can:\n",
    "- Make predictions using `w` and `b`\n",
    "- Measure how wrong the line is using MSE\n",
    "\n",
    "But how do we **improve `w` and `b`** so the line fits the data better?  \n",
    "We use a process called **Gradient Descent**, which is just a fancy way of saying:\n",
    "\n",
    "> ‚ÄúTry changing `w` and `b` little by little to make the error smaller.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "#### How it works (in simple words):\n",
    "\n",
    "1. **Look at the error**: How far is our line from the real points?  \n",
    "2. **Calculate direction**: Should we increase or decrease `w` and `b`?  \n",
    "3. **Take a small step**: Move `w` and `b` a little in the right direction.  \n",
    "4. **Repeat**: Keep adjusting until the line fits well.\n",
    "\n",
    "The size of each step we move is called the **learning rate** (we call it `lr` in code).  \n",
    "- Too small ‚Üí takes a long time to find the optimal w and b\n",
    "- Too big ‚Üí might overshoot and never fit well  \n",
    "\n",
    "---\n",
    "\n",
    "#### Formulas (don‚Äôt worry, just for reference):\n",
    "\n",
    "We adjust like this:\n",
    "\n",
    "$$\n",
    "w = w - \\text{learning rate} \\times dw\n",
    "$$\n",
    "$$\n",
    "b = b - \\text{learning rate} \\times db\n",
    "$$\n",
    "\n",
    "`dw` is basically how much the error would go up or down if we nudge w a tiny bit. `db` is the same for b.\n",
    "\n",
    "---\n",
    "\n",
    "# **3. Question**\n",
    "\n",
    "Train the Model\n",
    "\n",
    "We are going to improve `w` and `b` using Gradient Descent, we would set both at 0,.  \n",
    "\n",
    "- The code below already **uses your `predict()` function** inside the `compute_gradients()` and `mse()` functions.  \n",
    "- You **don‚Äôt need to call `predict()` directly**, just fill in:\n",
    "  - `lr` (learning rate)\n",
    "  - `epochs` (number of steps to repeat)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_5njX5MI7xYU"
   },
   "outputs": [],
   "source": [
    "# Gradient Descent Starter Code\n",
    "\n",
    "# Compute gradients for Linear Regression (basically calculates dw and db) (no need to edit this!)\n",
    "def compute_gradients(x_data, y_data, w, b):\n",
    "    \"\"\"\n",
    "    Calculates how much w and b should change to reduce error.\n",
    "    Uses the formula:\n",
    "        dw = average of (-2 * x * (y - y_pred))\n",
    "        db = average of (-2 * (y - y_pred))\n",
    "    \"\"\"\n",
    "    m = len(x_data)\n",
    "    dw, db = 0, 0\n",
    "    for i in range(m):\n",
    "        y_pred = predict(x_data[i], w, b)  # <-- uses the student's predict() function\n",
    "        dw += -2 * x_data[i] * (y_data[i] - y_pred)\n",
    "        db += -2 * (y_data[i] - y_pred)\n",
    "    return dw / m, db / m\n",
    "\n",
    "\n",
    "# Initial guesses for w and b\n",
    "w = 0\n",
    "b = 0\n",
    "\n",
    "# Learning rate\n",
    "lr = None    # replace with a small number like 0.01\n",
    "\n",
    "# Number of steps\n",
    "epochs = None   # you can try 50 or 100\n",
    "\n",
    "# Training loop (no need to edit this!)\n",
    "for i in range(epochs):\n",
    "    dw, db = compute_gradients(x_data, y_data, w, b)  # calculates how to change w and b\n",
    "    w -= lr * dw\n",
    "    b -= lr * db\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Step {i}: w={w:.2f}, b={b:.2f}, loss={mse(x_data,y_data,w,b):.2f}\")\n",
    "\n",
    "print(f\"\\nFinal line: y = {w:.2f}x + {b:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "C5lhaDtjt-gi"
   },
   "outputs": [],
   "source": [
    "## Lets look at how our learned model performs\n",
    "## Watch how close the red line is to the points ‚Äî the closer, the better the model learned.‚Äù\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predictions using trained w and b\n",
    "y_pred = [predict(xi, w, b) for xi in x_data]\n",
    "\n",
    "plt.scatter(x_data, y_data, color=\"blue\", label=\"Data points\")\n",
    "plt.plot(x_data, y_pred, color=\"red\", label=f\"Learned line: y={w:.2f}x + {b:.2f}\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Linear Regression Fit\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y_pXaBAa41bi"
   },
   "source": [
    "---\n",
    "### Logistic Regression (Predicting Classes 0 or 1)\n",
    "\n",
    "### The Idea  \n",
    "Sometimes we don‚Äôt want to predict numbers ‚Äî we want to **classify things**.  \n",
    "For example:\n",
    "\n",
    "| x | y |\n",
    "|---|---|\n",
    "| 1 | 0 |\n",
    "| 2 | 0 |\n",
    "| 3 | 1 |\n",
    "| 4 | 1 |\n",
    "\n",
    "- If `x` is small ‚Üí class 0  \n",
    "- If `x` is big ‚Üí class 1  \n",
    "\n",
    "We use **Logistic Regression** to find a ‚Äúline‚Äù that separates the two classes.  \n",
    "Instead of predicting a number, it predicts a **probability** between 0 and 1.  \n",
    "\n",
    "---\n",
    "\n",
    "###  Sigmoid Function  \n",
    "\n",
    "We use a special function called **sigmoid** to turn any number into a probability:  \n",
    "\n",
    "$$\n",
    "sigmoid(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "- Input `z` can be any number  \n",
    "- Output is always between 0 and 1  \n",
    "\n",
    "# **4. Question**\n",
    "Write the sigmoid function in code\n",
    "\n",
    "**Stater code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Oa2-ul0-8WHB"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid(z):\n",
    "    pass\n",
    "\n",
    "# Test it\n",
    "print(sigmoid(0))   # 0.5\n",
    "print(sigmoid(2))   # ~0.88\n",
    "print(sigmoid(-2))  # ~0.12\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hbtbijNF5w5v"
   },
   "source": [
    "---\n",
    "### Prediction Function\n",
    "\n",
    "Now to create the equation for logistic regression we combine the line formula you wrote in Question 1 (`wx + b`) with the **sigmoid** function you created:\n",
    "\n",
    "$$\n",
    "\\hat y = sigmoid(w \\cdot x + b)\n",
    "$$\n",
    "\n",
    "- `≈∑` is the **predicted probability** that `y = 1`.  \n",
    "- If `≈∑ > 0.5` ‚Üí predict class **1**  \n",
    "- If `≈∑ <= 0.5` ‚Üí predict class **0**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "L_dj3_uPuUc-"
   },
   "outputs": [],
   "source": [
    "def predict_prob(x, w, b):\n",
    "    z = predict(x, w, b)\n",
    "    return sigmoid(z)\n",
    "\n",
    "def predict_class(prob):\n",
    "    \"\"\"\n",
    "    Returns the predicted class (0 or 1) for input prob\n",
    "    \"\"\"\n",
    "    return 1 if prob > 0.5 else 0\n",
    "\n",
    "# Example usage\n",
    "print(predict_prob(2, 1, 0))  # probability for x=2\n",
    "print(predict_class(predict_prob(2, 1, 0)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zON3tFLABFd8"
   },
   "source": [
    "# **5. Question**\n",
    "\n",
    "Now that you have your `predict_prob()` function, let's use it to **calculate probabilities** and **class** for some new inputs.\n",
    "\n",
    "- Calculate probability for `x = 4, w = 2, b = 1`  \n",
    "- Calculate probability for `x = 1, w = 0.5, b = 1`  \n",
    "- Calculate probability for `x = 3, w = -2, b = 3`  \n",
    "\n",
    "**Hint:** Call your `predict_prob(x, w, b)` and `predict_class(prob)` functions for each case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fqVk-Gp66GSj"
   },
   "source": [
    "---\n",
    "\n",
    "### Loss Function: Log Loss\n",
    "\n",
    "Remember in **Linear Regression**, we used **Mean Squared Error (MSE)** to see how far off our predictions were:\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{m} \\sum (y_i - \\hat y_i)^2\n",
    "$$\n",
    "\n",
    "- Small MSE ‚Üí our line fits the data well  \n",
    "- Large MSE ‚Üí our line is far from the data  \n",
    "\n",
    "In **Logistic Regression**, we are predicting **probabilities** instead of exact numbers, so we need a different way to measure error: **Log Loss**.  \n",
    "\n",
    "### What is Log Loss?\n",
    "\n",
    "Log Loss tells us:  \n",
    "\n",
    "> ‚ÄúHow confident was the model when it made a wrong prediction?‚Äù  \n",
    "\n",
    "Formula:\n",
    "\n",
    "$$\n",
    "LogLoss = -\\frac{1}{m} \\sum \\Big[y \\cdot \\log(\\hat y) + (1-y) \\cdot \\log(1-\\hat y)\\Big]\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- `y` = actual class (0 or 1)  \n",
    "- `≈∑` = predicted probability that `y = 1`  \n",
    "\n",
    "Interpretation:  \n",
    "- **Small Log Loss ‚Üí good predictions** (predicted probabilities match actual classes)  \n",
    "- **Large Log Loss ‚Üí bad predictions** (predictions are far from the true class)  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "# **6. Question**\n",
    "\n",
    "Implement Log Loss\n",
    "\n",
    "Now we will **compute the Log Loss** for some example predictions.  \n",
    "\n",
    "You already have the `predict_prob(x, w, b)` function.  \n",
    "\n",
    "Since we haven‚Äôt trained the model yet, you can **try different values of `w` and `b`** to see how the Log Loss changes.  \n",
    "\n",
    "Example:  \n",
    "- Start with `w = 1, b = 0`  \n",
    "- Then try `w = 2, b = 1`  \n",
    "- Try other values and observe how the Log Loss changes  \n",
    "\n",
    "Use the dataset in the starter code\n",
    "\n",
    "**Starter Code:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "s49sFYec9e0a"
   },
   "outputs": [],
   "source": [
    "# Dataset\n",
    "x_data = [1, 2, 3, 4]\n",
    "y_data = [0, 0, 1, 1]\n",
    "\n",
    "\n",
    "def log_loss(x_data, y_data, w, b):\n",
    "    m = len(x_data)\n",
    "    total = 0\n",
    "    for i in range(m):\n",
    "        y_pred = predict_prob(x_data[i], w, b)\n",
    "        # avoid log(0)\n",
    "        y_pred = min(max(y_pred, 1e-10), 1-1e-10)\n",
    "        total += y_data[i]*math.log(y_pred) + (1-y_data[i])*math.log(1-y_pred)\n",
    "    return -total/m\n",
    "\n",
    "w = None\n",
    "b = None\n",
    "print(\"Log Loss:\", log_loss(x_data, y_data, w, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PbcmNyzS6atp"
   },
   "source": [
    "---\n",
    "### Gradients for Logistic Regression\n",
    "\n",
    "Just like in **Linear Regression**, we need a way to **update our parameters** `w` and `b` so that our predictions get better.  \n",
    "\n",
    "- `dw` tells us **how much to change `w`** to reduce Log Loss  \n",
    "- `db` tells us **how much to change `b`**  \n",
    "\n",
    "We calculate the **average change needed** across all data points:\n",
    "\n",
    "$$\n",
    "dw = \\frac{1}{m} \\sum (\\hat y - y) \\cdot x\n",
    "$$\n",
    "\n",
    "$$\n",
    "db = \\frac{1}{m} \\sum (\\hat y - y)\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- `y` = actual class (0 or 1)  \n",
    "- `≈∑` = predicted probability from `sigmoid(wx+b)`  \n",
    "- `m` = number of data points  \n",
    "\n",
    "**Lets relate this to Linear Regression:**  \n",
    "- In Linear Regression, we computed `dw = d(MSE)/dw`  \n",
    "- Here, we compute `dw = d(LogLoss)/dw`  \n",
    "- Same idea: **we look at how much the error changes with respect to each parameter**  \n",
    "\n",
    "### Train Logistic Regression\n",
    "\n",
    "Now that we can:\n",
    "\n",
    "- Predict probabilities using `predict_prob()`  \n",
    "- Measure error using `log_loss()`  \n",
    "- Compute gradients using `compute_gradients_logistic()`\n",
    "\n",
    "‚Ä¶it‚Äôs time to **train the model** using **Gradient Descent**.  \n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. Start with initial guesses for `w` and `b`  \n",
    "2. Choose a small learning rate `lr` (how big each step should be)  \n",
    "3. Repeat the update for several steps (`epochs`)  \n",
    "\n",
    "Each step:\n",
    "\n",
    "$$\n",
    "w = w - lr \\cdot dw\n",
    "$$\n",
    "\n",
    "$$\n",
    "b = b - lr \\cdot db\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "# **7. Question**\n",
    "Fill in `lr` and `epochs` and watch your model learn\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "DG6Mpn1m-5v3"
   },
   "outputs": [],
   "source": [
    "# Dataset\n",
    "x_data = [1, 2, 3, 4]\n",
    "y_data = [0, 0, 1, 1]\n",
    "\n",
    "# Initial guesses\n",
    "w = 0\n",
    "b = 0\n",
    "\n",
    "# Parameters to fill in\n",
    "lr = None      # learning rate, e.g., 0.1\n",
    "epochs = None   # number of steps, e.g., 100\n",
    "\n",
    "\n",
    "def compute_gradients_logistic(x_data, y_data, w, b):\n",
    "    m = len(x_data)\n",
    "    dw, db = 0, 0\n",
    "    for i in range(m):\n",
    "        y_pred = predict_prob(x_data[i], w, b)  # uses your predict_prob() function\n",
    "        dw += (y_pred - y_data[i]) * x_data[i]\n",
    "        db += (y_pred - y_data[i])\n",
    "    return dw/m, db/m\n",
    "\n",
    "\n",
    "# Training loop (no need to edit)\n",
    "for i in range(epochs):\n",
    "    dw, db = compute_gradients_logistic(x_data, y_data, w, b)\n",
    "    w -= lr * dw\n",
    "    b -= lr * db\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Step {i}: w={w:.2f}, b={b:.2f}, loss={log_loss(x_data,y_data,w,b):.2f}\")\n",
    "\n",
    "print(f\"\\nFinal model: probability = sigmoid({w:.2f}*x + {b:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "bdJ3rSzH6spF"
   },
   "outputs": [],
   "source": [
    "# Visualize Logistic Regression\n",
    "# In same manner we visualized linear regression model lets visualize the logistic regression model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_range = [i*0.1 for i in range(10, 41)]  # 1.0 to 4.0\n",
    "y_prob = [predict_prob(xi, w, b) for xi in x_range]\n",
    "\n",
    "plt.scatter(x_data, y_data, color=\"blue\", label=\"Data points\")\n",
    "plt.plot(x_range, y_prob, color=\"red\", label=\"Sigmoid curve\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"Probability of class 1\")\n",
    "plt.title(\"Logistic Regression Fit\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tzkz-cxg7Ey9"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 8. Questions\n",
    "\n",
    "1. For both Linear and logistic regression Try different learning rates (`lr`). How does it affect training?  \n",
    "2. What happens if you start with `w` and `b` far from the solution?  \n",
    "3. For `x = 2.5` in logistic regression, what is the predicted probability and class?  \n",
    "4. If you set `w = 0` in your Linear Regression model, what does the line look like? What does this tell you about the importance of `w`?  \n",
    "5. Compare Log Loss to MSE from Linear Regression: which one is easier to interpret? Why?\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
